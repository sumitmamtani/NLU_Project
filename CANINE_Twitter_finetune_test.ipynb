{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 12 23:14:41 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Quadro RTX 8000     On   | 00000000:2F:00.0 Off |                    0 |\r\n",
      "| N/A   53C    P0    61W / 250W |   3920MiB / 45556MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A    574808      C   ...on/3.8.6/intel/bin/python     3917MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_json_file = 'Reddit_Twitter_dataset/twitter/sarcasm_detection_shared_task_twitter_training.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "di = dict()\n",
    "\n",
    "with open(path_to_json_file, 'r') as j:\n",
    "     json_data = [json.loads(line) for line in j]\n",
    "\n",
    "i=0\n",
    "for tweet in json_data:\n",
    "    tot_cont=\"\"\n",
    "    for con in tweet['context']:\n",
    "      tot_cont+=(con + \" \")\n",
    "    tot_cont+=tweet['response']\n",
    "    lab = 0\n",
    "    if tweet['label'] == 'SARCASM':\n",
    "        lab=1\n",
    "    d1 = {'text': tot_cont, 'label': lab}\n",
    "\n",
    "    di[i] = d1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'text': \"A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it . @USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ðŸ’¯ @USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her ..\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(type(di))\n",
    "print(di[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CanineTokenizer, CanineForSequenceClassification\n",
    "\n",
    "tokenizer = CanineTokenizer.from_pretrained(\"google/canine-s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = []\n",
    "for i in di:\n",
    "  l = tokenizer(di[i][\"text\"], padding=\"max_length\", truncation=True)\n",
    "  l['labels'] = di[i][\"label\"]\n",
    "  tokenized_datasets.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch \n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(tokenized_datasets)\n",
    "\n",
    "train_data = tokenized_datasets[:4500]\n",
    "val_data = tokenized_datasets[4500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForSequenceClassification were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CanineForSequenceClassification.from_pretrained(\"google/canine-s\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mss9240/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4500\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4230\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4230' max='4230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4230/4230 1:13:09, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.502741</td>\n",
       "      <td>0.762000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.535100</td>\n",
       "      <td>0.468240</td>\n",
       "      <td>0.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.535100</td>\n",
       "      <td>0.557148</td>\n",
       "      <td>0.742000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.321800</td>\n",
       "      <td>0.684472</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.321800</td>\n",
       "      <td>0.849788</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.151800</td>\n",
       "      <td>1.042805</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.151800</td>\n",
       "      <td>1.135417</td>\n",
       "      <td>0.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>1.324319</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>1.268973</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>1.365285</td>\n",
       "      <td>0.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>1.424352</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>1.430069</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>1.491058</td>\n",
       "      <td>0.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>1.486962</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.504670</td>\n",
       "      <td>0.788000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-282\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-282/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-282/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-282/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-282/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-564\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-564/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-564/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-564/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-564/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-846\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-846/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-846/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-846/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-846/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-1128\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-1128/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-1128/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-1128/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-1128/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-1410\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-1410/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-1410/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-1410/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-1410/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-1692\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-1692/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-1692/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-1692/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-1692/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-1974\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-1974/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-1974/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-1974/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-1974/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-2256\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-2256/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-2256/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-2256/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-2256/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-2538\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-2538/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-2538/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-2538/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-2538/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-2820\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-2820/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-2820/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-2820/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-2820/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-3102\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-3102/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-3102/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-3102/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-3102/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-3384\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-3384/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-3384/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-3384/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-3384/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-3666\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-3666/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-3666/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-3666/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-3666/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-3948\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-3948/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-3948/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-3948/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-3948/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to test_trainer/new_ep/copy_twitter/checkpoint-4230\n",
      "Configuration saved in test_trainer/new_ep/copy_twitter/checkpoint-4230/config.json\n",
      "Model weights saved in test_trainer/new_ep/copy_twitter/checkpoint-4230/pytorch_model.bin\n",
      "tokenizer config file saved in test_trainer/new_ep/copy_twitter/checkpoint-4230/tokenizer_config.json\n",
      "Special tokens file saved in test_trainer/new_ep/copy_twitter/checkpoint-4230/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4230, training_loss=0.14062255808365823, metrics={'train_runtime': 4390.831, 'train_samples_per_second': 15.373, 'train_steps_per_second': 0.963, 'total_flos': 8.86724195328e+16, 'train_loss': 0.14062255808365823, 'epoch': 15.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer/new_ep/copy_twitter\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'Reddit_Twitter_dataset/twitter/sarcasm_detection_shared_task_twitter_testing.jsonl'\n",
    "\n",
    "di_test = dict()\n",
    "\n",
    "with open(test_file, 'r') as j:\n",
    "     test_data = [json.loads(line) for line in j]\n",
    "\n",
    "i=0\n",
    "for tweet in test_data:\n",
    "    tot_cont=\"\"\n",
    "    for con in tweet['context']:\n",
    "      tot_cont+=(con + \" \")\n",
    "    tot_cont+=tweet['response']\n",
    "    lab = 0\n",
    "    if tweet['label'] == 'SARCASM':\n",
    "        lab=1\n",
    "    d1 = {'text': tot_cont, 'label': lab}\n",
    "\n",
    "    di_test[i] = d1\n",
    "    i+=1\n",
    "\n",
    "    \n",
    "tokenized_datasets_test = []\n",
    "for i in di_test:\n",
    "  l = tokenizer(di_test[i][\"text\"], padding=\"max_length\", truncation=True)\n",
    "  l['labels'] = di_test[i][\"label\"]\n",
    "  tokenized_datasets_test.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file test_trainer/new_ep/copy_twitter/4ep_best_run/config.json\n",
      "Model config CanineConfig {\n",
      "  \"_name_or_path\": \"google/canine-s\",\n",
      "  \"architectures\": [\n",
      "    \"CanineForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 57344,\n",
      "  \"downsampling_rate\": 4,\n",
      "  \"eos_token_id\": 57345,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"local_transformer_stride\": 128,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"model_type\": \"canine\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hash_buckets\": 16384,\n",
      "  \"num_hash_functions\": 8,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 16,\n",
      "  \"upsampling_kernel_size\": 4,\n",
      "  \"use_cache\": true\n",
      "}\n",
      "\n",
      "loading weights file test_trainer/new_ep/copy_twitter/4ep_best_run/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing CanineForSequenceClassification.\n",
      "\n",
      "All the weights of CanineForSequenceClassification were initialized from the model checkpoint at test_trainer/new_ep/copy_twitter/4ep_best_run.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CanineForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"test_trainer/new_ep/copy_twitter/4ep_best_run\"\n",
    "model = CanineForSequenceClassification.from_pretrained(model_path, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "test_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8127884864807129,\n",
       " 'eval_accuracy': 0.7287853577371048,\n",
       " 'eval_runtime': 15.9934,\n",
       " 'eval_samples_per_second': 37.578,\n",
       " 'eval_steps_per_second': 2.376}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trainer.evaluate(eval_dataset = tokenized_datasets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
